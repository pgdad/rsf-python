# Tutorial 4: Deploying to AWS with `rsf deploy`

## What You'll Learn

In this tutorial you will:

- Deploy a generated RSF workflow to AWS using Terraform
- Understand the 6 generated Terraform files that make up the infrastructure module
- See the full deploy pipeline in action: code generation, Terraform init, Terraform apply
- Verify the deployment succeeded using the AWS CLI

---

## Prerequisites

- Completed Tutorials 1-3: you have a `my-workflow/` directory with a validated `workflow.yaml` and generated code (orchestrator + handlers)
- **AWS CLI** installed and configured with credentials that have permissions to create Lambda functions, IAM roles, and CloudWatch log groups
- **Terraform CLI** installed (version 1.0 or newer)
- An AWS account with the `us-east-2` region enabled

Verify your tools are ready:

```bash
aws sts get-caller-identity
```

You should see your AWS account ID and ARN. If this fails, configure the AWS CLI with `aws configure`.

```bash
terraform --version
```

You should see `Terraform v1.x.x` or newer. If terraform is not found, install it from [terraform.io](https://terraform.io).

> **Cost warning:** This tutorial deploys real AWS infrastructure. A Lambda function, IAM role, and CloudWatch log group will be created in your account. These resources incur costs while they exist. Tutorial 5 shows you how to tear everything down cleanly.

---

## Step 1: Prepare the Workflow

You should have the order processing workflow from Tutorial 3 in `workflow.yaml`. If not, make sure you are in the `my-workflow/` directory and replace the contents of `workflow.yaml` with:

```yaml
rsf_version: "1.0"
Comment: "Order processing workflow"
StartAt: ValidateOrder

States:
  ValidateOrder:
    Type: Task
    Next: CheckAmount

  CheckAmount:
    Type: Choice
    Choices:
      - Variable: "$.amount"
        NumericGreaterThan: 100
        Next: RequireApproval
    Default: ProcessOrder

  RequireApproval:
    Type: Task
    Next: ProcessOrder

  ProcessOrder:
    Type: Task
    End: true
```

Also make sure the handlers have been generated. If you have not run `rsf generate` since adding this workflow, run it now:

```bash
rsf generate
```

You should see handler stubs created for `ValidateOrder`, `RequireApproval`, and `ProcessOrder`.

---

## Step 2: Deploy

Run `rsf deploy` with the `--auto-approve` flag to skip the interactive Terraform confirmation:

```bash
rsf deploy --auto-approve
```

The `rsf deploy` command does three things in sequence:

1. **Generates code** — runs the same codegen pipeline as `rsf generate` (orchestrator + handler stubs)
2. **Generates Terraform** — creates 6 HCL files in a `terraform/` subdirectory
3. **Runs Terraform** — executes `terraform init` then `terraform apply` to provision AWS resources

Expected output (simplified):

```
Code generated: orchestrator.py + 3 handler(s) (3 skipped)
Terraform generated: 6 file(s) in terraform (0 skipped)

Running terraform init...
Initializing the backend...
...
Terraform has been successfully initialized!

Running terraform apply...
...
Apply complete! Resources: 4 added, 0 changed, 0 destroyed.

Deploy complete
```

The 4 resources created are: the Lambda function, the IAM role, the IAM role policy, and the CloudWatch log group.

> The `--auto-approve` (or `-y`) flag tells Terraform to apply without asking "Do you want to perform these actions?" If you omit it, Terraform will show the plan and prompt you to confirm.

---

## Step 3: Explore the Generated Terraform

Your project directory now includes a `terraform/` subdirectory:

```
my-workflow/
├── orchestrator.py
├── workflow.yaml
├── pyproject.toml
├── .gitignore
├── handlers/
│   ├── __init__.py
│   ├── example_handler.py
│   ├── validate_order.py
│   ├── require_approval.py
│   └── process_order.py
├── terraform/                    <-- NEW
│   ├── main.tf
│   ├── variables.tf
│   ├── iam.tf
│   ├── outputs.tf
│   ├── cloudwatch.tf
│   ├── backend.tf
│   ├── .terraform/               (Terraform providers)
│   ├── .terraform.lock.hcl       (Terraform lock file)
│   └── terraform.tfstate         (Terraform state — do not commit)
└── tests/
    ├── __init__.py
    └── test_example.py
```

The 6 generated files form a complete Terraform module. Each is explained below.

### terraform/main.tf — Lambda Function

```hcl
# DO NOT EDIT - Generated by RSF

locals {
  function_name = "${var.name_prefix}-${var.workflow_name}"
}

data "archive_file" "lambda_zip" {
  type        = "zip"
  source_dir  = "${var.source_dir}/src"
  output_path = "${path.module}/Order processing workflow.zip"
}

resource "aws_lambda_function" "order_processing_workflow" {
  function_name    = local.function_name
  handler          = "generated.orchestrator.lambda_handler"
  runtime          = "python3.13"
  role             = aws_iam_role.lambda_exec.arn
  filename         = data.archive_file.lambda_zip.output_path
  source_code_hash = data.archive_file.lambda_zip.output_base64sha256
  timeout          = var.timeout
  memory_size      = var.memory_size

  durable_config {
    execution_timeout = var.execution_timeout
    retention_period  = var.retention_period
  }

  lifecycle {
    ignore_changes = [filename, source_code_hash]
  }
}
```

Key elements:

- **`data.archive_file.lambda_zip`** — zips the source directory into a deployment package. Terraform handles this automatically.
- **`aws_lambda_function`** — creates the Lambda function with the Python 3.13 runtime. The `handler` points to `generated.orchestrator.lambda_handler` — the orchestrator file generated by `rsf generate`.
- **`durable_config`** — enables Lambda Durable Functions. The `execution_timeout` controls how long a durable execution can run (default: 24 hours). The `retention_period` controls how long completed execution history is kept (default: 14 days).

### terraform/variables.tf — Input Variables

```hcl
# DO NOT EDIT - Generated by RSF

variable "aws_region" {
  description = "AWS region for deployment"
  type        = string
  default     = "us-east-1"
}

variable "name_prefix" {
  description = "Prefix for all resource names"
  type        = string
  default     = "rsf"
}

variable "workflow_name" {
  description = "Workflow name used in resource naming"
  type        = string
  default     = "Order processing workflow"
}

variable "timeout" {
  description = "Lambda function timeout in seconds"
  type        = number
  default     = 900
}

variable "memory_size" {
  description = "Lambda function memory in MB"
  type        = number
  default     = 256
}

variable "source_dir" {
  description = "Path to the project source directory"
  type        = string
  default     = ".."
}

variable "log_retention_days" {
  description = "CloudWatch log retention in days"
  type        = number
  default     = 14
}

variable "execution_timeout" {
  description = "Durable execution timeout in seconds (1 to 31622400)"
  type        = number
  default     = 86400
}

variable "retention_period" {
  description = "Durable execution retention period in days (1 to 90)"
  type        = number
  default     = 14
}
```

All variables have sensible defaults. The most important ones:

- **`aws_region`** — where to deploy (override with `-var="aws_region=us-east-2"` if needed)
- **`name_prefix`** — prefixed to the function name (default: `rsf`), giving a function name like `rsf-Order processing workflow`
- **`workflow_name`** — derived from the `Comment` field in `workflow.yaml`
- **`timeout`** — Lambda invocation timeout in seconds (default: 900 = 15 minutes)
- **`execution_timeout`** — how long a durable execution can run (default: 86400 = 24 hours)

### terraform/iam.tf — IAM Role and Policy

```hcl
# DO NOT EDIT - Generated by RSF

resource "aws_iam_role" "lambda_exec" {
  name = "${local.function_name}-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Action = "sts:AssumeRole"
        Effect = "Allow"
        Principal = {
          Service = "lambda.amazonaws.com"
        }
      }
    ]
  })
}

resource "aws_iam_role_policy" "lambda_policy" {
  name = "${local.function_name}-policy"
  role = aws_iam_role.lambda_exec.id

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Sid    = "CloudWatchLogs"
        Effect = "Allow"
        Action = [
          "logs:CreateLogGroup",
          "logs:CreateLogStream",
          "logs:PutLogEvents"
        ]
        Resource = "arn:aws:logs:*:*:*"
      },
      {
        Sid    = "LambdaSelfInvoke"
        Effect = "Allow"
        Action = [
          "lambda:InvokeFunction"
        ]
        Resource = aws_lambda_function.order_processing_workflow.arn
      },
      {
        Sid    = "DurableExecution"
        Effect = "Allow"
        Action = [
          "lambda:CheckpointDurableExecution",
          "lambda:GetDurableExecution",
          "lambda:ListDurableExecutionsByFunction"
        ]
        Resource = aws_lambda_function.order_processing_workflow.arn
      }
    ]
  })
}
```

The IAM role and policy grant the Lambda function exactly the permissions it needs:

1. **CloudWatchLogs** — create log groups and streams, write log events. Required for Lambda to log output.
2. **LambdaSelfInvoke** — invoke itself. Required for the durable execution runtime to resume after checkpoints.
3. **DurableExecution** — checkpoint, get, and list durable executions. Required for the Lambda Durable Functions runtime to manage execution state.

The `assume_role_policy` allows the Lambda service to assume this role.

### terraform/outputs.tf — Terraform Outputs

```hcl
# DO NOT EDIT - Generated by RSF

output "function_arn" {
  description = "ARN of the Lambda function"
  value       = aws_lambda_function.order_processing_workflow.arn
}

output "function_name" {
  description = "Name of the Lambda function"
  value       = aws_lambda_function.order_processing_workflow.function_name
}

output "role_arn" {
  description = "ARN of the IAM role"
  value       = aws_iam_role.lambda_exec.arn
}

output "log_group_name" {
  description = "Name of the CloudWatch log group"
  value       = aws_cloudwatch_log_group.lambda_logs.name
}
```

These outputs are printed after `terraform apply` completes and can be referenced by other Terraform modules. The function ARN and name are particularly useful for invoking the Lambda and setting up event sources.

### terraform/cloudwatch.tf — CloudWatch Log Group

```hcl
# DO NOT EDIT - Generated by RSF

resource "aws_cloudwatch_log_group" "lambda_logs" {
  name              = "/aws/lambda/${local.function_name}"
  retention_in_days = var.log_retention_days
}
```

Creates a CloudWatch log group for the Lambda function with a configurable retention period (default: 14 days). Logs older than the retention period are automatically deleted.

### terraform/backend.tf — Remote State Backend

```hcl
# DO NOT EDIT - Generated by RSF

# No remote backend configured. State is stored locally.
# To enable S3 backend, configure backend_bucket in your RSF project.
```

By default, Terraform stores state in a local `terraform.tfstate` file. For team workflows or production use, you can configure an S3 backend for shared state. The local default is fine for this tutorial.

> **Important:** The `terraform.tfstate` file contains your infrastructure state and may include sensitive values. It is excluded from git by the `.gitignore` created by `rsf init`. Never commit it to version control.

---

## Step 4: Verify the Deployment

Check that the Lambda function was created using the AWS CLI:

```bash
aws lambda get-function \
  --function-name rsf-Order-processing-workflow \
  --region us-east-2 \
  --query 'Configuration.{FunctionName:FunctionName,Runtime:Runtime,Handler:Handler,State:State}'
```

Expected output:

```json
{
    "FunctionName": "rsf-Order-processing-workflow",
    "Runtime": "python3.13",
    "Handler": "generated.orchestrator.lambda_handler",
    "State": "Active"
}
```

This confirms that the Lambda function exists, uses the Python 3.13 runtime, and points to the generated orchestrator as its handler.

You can also check the Terraform outputs:

```bash
terraform -chdir=terraform output
```

Expected output:

```
function_arn = "arn:aws:lambda:us-east-2:123456789012:function:rsf-Order-processing-workflow"
function_name = "rsf-Order-processing-workflow"
log_group_name = "/aws/lambda/rsf-Order-processing-workflow"
role_arn = "arn:aws:iam::123456789012:role/rsf-Order-processing-workflow-role"
```

> The account ID (`123456789012`) and exact ARN values will differ in your output.

---

## What's Next

Continue to **Tutorial 5: Iterate, Invoke, and Tear Down**.

Your workflow is now live on AWS. Tutorial 5 shows you how to:

- Use `rsf deploy --code-only` to rapidly update handler logic without re-running Terraform
- Invoke the deployed Lambda with a test payload and see the workflow execute
- Tear down all infrastructure with `terraform destroy` — leaving zero orphaned resources

> **Reminder:** The infrastructure you deployed in this tutorial is live and may incur costs. Tutorial 5 shows you how to tear it down. If you are not continuing to Tutorial 5 immediately, run `cd terraform && terraform destroy -auto-approve` to remove all resources.
