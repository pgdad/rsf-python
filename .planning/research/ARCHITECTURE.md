# Architecture Research

**Domain:** Examples and integration testing for an existing Python CLI tool (RSF)
**Researched:** 2026-02-26
**Confidence:** HIGH — all findings based on direct inspection of existing codebase

## Standard Architecture

### System Overview

```
examples/
  order-processing/          <-- one self-contained example
    workflow.yaml            <-- RSF DSL (input to rsf generate + rsf deploy)
    handlers/                <-- user-owned handler code (@state decorated)
    src/                     <-- source layout expected by main.tf data.archive_file
      generated/             <-- rsf generate output (orchestrator.py)
      handlers/              <-- handlers copied here for packaging
    terraform/               <-- rsf deploy output (6 HCL files + tfstate)
    tests/
      test_local.py          <-- local mock-SDK execution tests
  data-pipeline/
    ... (same layout)

tests/
  test_examples/             <-- new: AWS integration test runner
    conftest.py              <-- boto3 helpers, fixtures
    test_order_processing.py <-- deploy→invoke→verify→teardown per example
    test_data_pipeline.py

scripts/
  run_integration_tests.sh   <-- single-command: deploy all, test all, teardown all
```

The existing source tree (`src/rsf/`) is not modified. All new components
live in `examples/`, `tests/test_examples/`, and `scripts/`.

### Component Responsibilities

| Component | Responsibility | Implementation |
|-----------|----------------|----------------|
| `examples/<name>/workflow.yaml` | DSL definition; input to existing `rsf generate` and `rsf deploy` | Hand-authored RSF YAML using existing DSL |
| `examples/<name>/handlers/` | User-owned handler code with `@state` decorators and structured logging | Plain Python files, not generated |
| `examples/<name>/terraform/` | Generated by `rsf deploy`; 6 HCL files + local tfstate | Output of existing `TerraformConfig` and Jinja2 templates |
| `examples/<name>/src/` | Lambda deployment package root; structured as `src/generated/` + `src/handlers/` | Required by `main.tf` `source_dir` variable |
| `tests/test_examples/` | Pytest suite that calls boto3 to invoke, poll, and verify each deployed example | New Python test files using existing `rsf.inspect.client` |
| `scripts/run_integration_tests.sh` | Orchestrates deploy→test→teardown lifecycle with env setup | Bash; wraps pytest and rsf CLI calls |

## Recommended Project Structure

```
examples/
  order-processing/
    workflow.yaml              # RSF DSL — defines the state machine
    handlers/                  # user-authored handler implementations
      __init__.py
      validate_order.py        # @state("ValidateOrder") def validate_order(...)
      process_payment.py       # @state("ProcessPayment") def process_payment(...)
      ... (one file per Task state)
    src/                       # Lambda package root (matches main.tf source_dir default "..")
      generated/               # rsf generate writes orchestrator.py here
        orchestrator.py        # DO NOT EDIT — generated by RSF
      handlers/                # copy of handlers/ for Lambda packaging
        __init__.py
        validate_order.py
        ...
    terraform/                 # rsf deploy writes Terraform files here
      main.tf
      variables.tf
      iam.tf
      outputs.tf
      cloudwatch.tf
      backend.tf
      terraform.tfstate         # local state; .gitignored
    tests/
      test_local.py             # local execution via MockDurableContext (no AWS)
    README.md                   # what this example demonstrates
  data-pipeline/
    ... (same layout)
  dynamodb-crud/
    ... (same layout, adds DynamoDB IAM to iam.tf override)

tests/
  test_examples/
    __init__.py
    conftest.py                 # shared: aws_region, function_arn_for fixture, log group name, poll helpers
    test_order_processing.py
    test_data_pipeline.py
    test_dynamodb_crud.py

scripts/
  run_integration_tests.sh     # export AWS env, cd into each example, rsf deploy -y, pytest, rsf destroy
```

### Structure Rationale

- **`examples/<name>/src/` layout:** The existing `main.tf.j2` template packages Lambda from `${var.source_dir}/src`. The default `source_dir` is `".."` (one level above `terraform/`). So the package root is `examples/<name>/src/`. Generated orchestrator goes in `src/generated/` and handlers in `src/handlers/`. This matches the existing `handler = "generated.orchestrator.lambda_handler"` in `main.tf.j2` with zero changes to templates.

- **`examples/<name>/terraform/` layout:** The `rsf deploy` command uses `--tf-dir terraform` by default. Running `rsf deploy workflow.yaml -y` from inside `examples/<name>/` produces Terraform files in `examples/<name>/terraform/` and local state in `examples/<name>/terraform/terraform.tfstate`. Each example gets isolated state — no shared backend needed.

- **`tests/test_examples/` separate from `examples/<name>/tests/`:** Local (mock SDK) tests live inside each example directory to keep the example self-contained. AWS integration tests that require deployed infrastructure live in the project-level `tests/test_examples/` alongside existing `tests/test_integration/`. This mirrors the existing test organization.

- **`scripts/` for lifecycle orchestration:** A shell script keeps the integration test runner separate from the pytest files. pytest files assert correctness; the shell script manages the deploy→teardown lifecycle and CI invocation.

## Architectural Patterns

### Pattern 1: Example as a Self-Contained RSF Project

**What:** Each example directory is a complete, standalone RSF project. It has a `workflow.yaml`, handler implementations, and a `terraform/` directory — identical to what a user would create with `rsf init`. The example is the demo artifact AND the test fixture.

**When to use:** Always. Every example must be runnable end-to-end with `rsf deploy` without any special tooling.

**Trade-offs:** The `src/` layout is slightly non-obvious (two levels: `src/generated/` and `src/handlers/`) but is dictated by the existing `main.tf.j2` template. Do not change the template; adapt the example layout to match it.

**Example — running an example manually:**
```bash
cd examples/order-processing
rsf generate workflow.yaml --output src/generated
cp -r handlers/ src/handlers/
rsf deploy workflow.yaml --tf-dir terraform -y
```

The integration test harness automates exactly this sequence.

### Pattern 2: Dual Verification (Return Value + CloudWatch Logs)

**What:** AWS integration tests verify correctness through two independent channels:
1. **Lambda return value** — poll `list_durable_executions_by_function` until SUCCEEDED, then check the final output payload against expected assertions.
2. **CloudWatch Logs** — query the log group `/aws/lambda/rsf-<workflow-name>` for structured JSON log entries emitted by handlers during execution.

**When to use:** Return value verification catches workflow output correctness. CloudWatch log verification catches intermediate state correctness (handler inputs, branching decisions, retry events).

**Trade-offs:** CloudWatch Logs queries add latency (logs may take a few seconds to appear after execution completes). Use a short retry loop (3 attempts, 2s sleep) in the log verification step.

**Example — verification helper:**
```python
import boto3, json, time

def get_execution_result(function_name: str, execution_id: str, region: str) -> dict:
    """Poll until execution completes and return the output payload."""
    client = boto3.client("lambda", region_name=region)
    for _ in range(60):  # max 120s
        resp = client.get_durable_execution(
            FunctionName=function_name,
            ExecutionId=execution_id,
        )
        status = resp["DurableExecution"]["Status"]
        if status == "SUCCEEDED":
            return json.loads(resp["DurableExecution"]["Output"])
        if status in ("FAILED", "TIMED_OUT", "STOPPED"):
            raise AssertionError(f"Execution {status}: {resp}")
        time.sleep(2)
    raise TimeoutError("Execution did not complete within 120s")

def get_log_entries(log_group: str, execution_id: str, region: str) -> list[dict]:
    """Fetch structured log entries for a specific execution from CloudWatch."""
    logs = boto3.client("logs", region_name=region)
    # Short retry loop — logs may lag execution completion by 2-5s
    for attempt in range(3):
        response = logs.filter_log_events(
            logGroupName=log_group,
            filterPattern=f'{{ $.execution_id = "{execution_id}" }}',
        )
        if response["events"]:
            return [json.loads(e["message"]) for e in response["events"]]
        time.sleep(2)
    return []
```

### Pattern 3: Handler Structured Logging Convention

**What:** Every handler emits a single structured JSON log line at INFO level on entry and exit. The log entry includes `execution_id`, `state_name`, `phase` (enter/exit), and relevant input/output fields. This makes CloudWatch log queries deterministic.

**When to use:** All handlers in all examples. The convention is enforced by the test harness (tests assert log entries exist and have correct shape).

**Trade-offs:** Adds ~5 lines of boilerplate per handler. The tradeoff is worthwhile: tests become deterministic and self-documenting.

**Example — handler logging convention:**
```python
import json, logging, os
from rsf.registry import state

logger = logging.getLogger(__name__)

@state("ValidateOrder")
def validate_order(input_data: dict) -> dict:
    execution_id = os.environ.get("AWS_LAMBDA_LOG_STREAM_NAME", "local")
    logger.info(json.dumps({
        "execution_id": execution_id,
        "state_name": "ValidateOrder",
        "phase": "enter",
        "order_id": input_data.get("order_id"),
    }))
    # ... business logic ...
    result = {"validated": True, "order_id": input_data["order_id"]}
    logger.info(json.dumps({
        "execution_id": execution_id,
        "state_name": "ValidateOrder",
        "phase": "exit",
        "result": result,
    }))
    return result
```

Note: `AWS_LAMBDA_LOG_STREAM_NAME` is not the durable execution ID. For examples, include the durable execution ID by accepting it as a workflow input field and threading it through handler inputs via DSL I/O processing (e.g., `Parameters` with `execution_id.$: "$.execution_id"`). The test harness passes `execution_id` as part of the invocation payload.

### Pattern 4: Terraform State Isolation Per Example

**What:** Each example uses a local Terraform state file in `examples/<name>/terraform/terraform.tfstate`. No S3 backend is configured. Teardown runs `terraform destroy -auto-approve` from the `terraform/` directory.

**When to use:** Always. Integration tests are run by individual developers or in CI against a single shared AWS account. Local state per example prevents conflicts between concurrent test runs on the same machine and avoids the need for a shared S3 bucket.

**Trade-offs:** Local state is not shared. If the developer's machine is lost, manual AWS cleanup is required. For a test harness on a developer tool, this tradeoff is correct — simplicity over durability.

**Example — teardown in test:**
```python
import subprocess, pathlib

def teardown_example(example_dir: pathlib.Path):
    subprocess.run(
        ["terraform", "destroy", "-auto-approve"],
        cwd=example_dir / "terraform",
        check=True,
    )
```

### Pattern 5: Invoke via Event Type (Async), Poll for Completion

**What:** Lambda Durable Functions must be invoked with `InvocationType="Event"` (async). The test harness invokes, gets an execution ID from the response or by listing executions, then polls `list_durable_executions_by_function` until terminal status.

**When to use:** All integration tests. Synchronous invocation (`RequestResponse`) is not supported for durable functions.

**Trade-offs:** Polling adds test latency. Use a 2-second polling interval with a 60-attempt ceiling (120s maximum wait). Most examples complete in under 10 seconds.

**Example — async invoke pattern:**
```python
def invoke_example(function_name: str, payload: dict, region: str) -> str:
    """Invoke a durable Lambda function and return the execution ID."""
    client = boto3.client("lambda", region_name=region)
    # Async invocation — durable functions require Event type
    client.invoke(
        FunctionName=function_name,
        InvocationType="Event",
        Payload=json.dumps(payload).encode(),
    )
    # List recent executions to find the one we just started
    time.sleep(1)  # brief settle time
    executions = client.list_durable_executions_by_function(
        FunctionName=function_name,
        MaxItems=5,
    )
    # Return the most recent execution ID
    return executions["DurableExecutions"][0]["ExecutionId"]
```

## Data Flow

### Deploy Flow (per example)

```
Developer runs: rsf deploy examples/order-processing/workflow.yaml --tf-dir terraform -y
    |
    v
rsf.dsl.parser.load_definition(workflow.yaml)
    --> StateMachineDefinition (Pydantic model)
    |
    v
rsf.codegen.generator.generate(definition, workflow.yaml, output_dir=examples/order-processing/src/generated)
    --> examples/order-processing/src/generated/orchestrator.py  (ALWAYS overwritten — generated marker)
    --> examples/order-processing/src/generated/handlers/__init__.py
    --> handler stubs SKIPPED (user handlers already exist in handlers/)
    |
    v
rsf.terraform.generator.generate_terraform(TerraformConfig(workflow_name=...), output_dir=examples/order-processing/terraform)
    --> examples/order-processing/terraform/{main.tf, variables.tf, iam.tf, outputs.tf, cloudwatch.tf, backend.tf}
    |
    v
subprocess: terraform init (in examples/order-processing/terraform/)
    |
    v
subprocess: terraform apply -auto-approve (in examples/order-processing/terraform/)
    --> AWS Lambda function created: rsf-order-processing
    --> CloudWatch log group: /aws/lambda/rsf-order-processing
    --> IAM role: rsf-order-processing-role
    --> Local state: examples/order-processing/terraform/terraform.tfstate
```

### Integration Test Flow (per example)

```
pytest tests/test_examples/test_order_processing.py
    |
    v
[setup] Read terraform output: function_name, log_group_name
    (terraform output -json in examples/order-processing/terraform/)
    |
    v
[test] Invoke Lambda (Event type) with test payload including execution_id field
    --> boto3.client("lambda").invoke(InvocationType="Event", ...)
    |
    v
[poll] Poll list_durable_executions_by_function until SUCCEEDED (max 120s, 2s interval)
    |
    v
[verify - channel 1] Assert on Lambda return value (output payload)
    --> json.loads(execution["Output"]) == expected_output
    |
    v
[verify - channel 2] Query CloudWatch Logs for structured entries
    --> logs.filter_log_events(logGroupName=log_group, filterPattern='{ $.execution_id = "..." }')
    --> Assert state_name, phase, result fields present in log entries
    |
    v
[teardown] terraform destroy -auto-approve
```

### CloudWatch Log Group Naming

The log group name is determined by the existing `cloudwatch.tf.j2` template:

```hcl
resource "aws_cloudwatch_log_group" "lambda_logs" {
  name = "/aws/lambda/${local.function_name}"
}
```

`local.function_name` = `${var.name_prefix}-${var.workflow_name}` = `rsf-<workflow-name>`.

For example `order-processing`: `/aws/lambda/rsf-order-processing`.

The test harness reads this from `terraform output -json` to avoid hardcoding it:

```python
import subprocess, json

def get_tf_outputs(terraform_dir: pathlib.Path) -> dict:
    result = subprocess.run(
        ["terraform", "output", "-json"],
        cwd=terraform_dir,
        capture_output=True,
        text=True,
        check=True,
    )
    raw = json.loads(result.stdout)
    return {k: v["value"] for k, v in raw.items()}

# outputs["log_group_name"] == "/aws/lambda/rsf-order-processing"
# outputs["function_name"] == "rsf-order-processing"
# outputs["function_arn"] == "arn:aws:lambda:..."
```

### Verification Script Architecture

```
tests/test_examples/
    conftest.py
        - Fixtures: aws_region (from env), example_dir(name) -> Path
        - Fixture: tf_outputs(example_dir) -> dict (runs terraform output)
        - Fixture: function_name(tf_outputs) -> str
        - Fixture: log_group_name(tf_outputs) -> str
        - Helper: poll_execution(function_name, execution_id, region, timeout=120) -> dict
        - Helper: invoke_async(function_name, payload, region) -> str (execution_id)
        - Helper: query_logs(log_group, execution_id, region, retries=3) -> list[dict]

    test_order_processing.py
        - test_happy_path(): invoke standard order, verify output + logs
        - test_high_value_approval_path(): invoke order>5000, verify Choice branch taken
        - test_payment_retry(): invoke with retry-triggering payload, verify retry logs
        - test_failed_order(): invoke with invalid payload, verify FAILED status + error

    test_data_pipeline.py
        - test_map_processes_all_items(): verify Map output contains all processed items
        - test_parallel_branches_complete(): verify Parallel result array length

    test_dynamodb_crud.py
        - test_write_and_read(): verify DynamoDB item written and read back via workflow
```

## Scaling Considerations

This is a developer test harness, not a production service. Scaling here means "how to add more examples" not "more traffic."

| Scale | Approach |
|-------|----------|
| 1-3 examples | Current architecture sufficient. All examples deploy and teardown sequentially. |
| 4-10 examples | Add `--example=<name>` flag to `run_integration_tests.sh` to select a subset. Sequential deploy is fine (each takes ~30-60s). |
| 10+ examples | Parallelize deploy/teardown across examples in the shell script. AWS account Lambda limits become relevant — use separate AWS accounts for CI if needed. |

### Scaling Priorities

1. **First bottleneck:** Deploy/teardown time. Each `terraform apply` takes ~30-60 seconds. With 5 examples, total time is 3-5 minutes. Acceptable for CI. If time exceeds 10 minutes, parallelize deploys.
2. **Second bottleneck:** AWS Lambda concurrency limits. Lambda Durable Functions may have lower concurrency limits than standard Lambda. Check account limits before adding many concurrent integration test runs.

## Anti-Patterns

### Anti-Pattern 1: Shared Terraform State Across Examples

**What people do:** Use one Terraform state file (or S3 backend) for all examples with separate modules.
**Why it's wrong:** A single failing example blocks teardown of all examples. State file conflicts occur when two developers run tests concurrently. More complex to debug.
**Do this instead:** One local `terraform.tfstate` per example in `examples/<name>/terraform/`. Teardown each example independently.

### Anti-Pattern 2: Hardcoding Lambda Function Names in Tests

**What people do:** Assert against `"rsf-order-processing"` or `/aws/lambda/rsf-order-processing"` directly in test code.
**Why it's wrong:** If `name_prefix` or `workflow_name` change in TerraformConfig, tests silently break. Tests also fail if the same example is deployed with a different prefix.
**Do this instead:** Read function name and log group name from `terraform output -json` at test setup. The `outputs.tf.j2` template already exports `function_name` and `log_group_name`.

### Anti-Pattern 3: Polling with Synchronous Lambda Invocation

**What people do:** Call `invoke(InvocationType="RequestResponse")` and wait for the response.
**Why it's wrong:** Lambda Durable Functions require async invocation (`InvocationType="Event"`). Synchronous invocation returns immediately without executing the durable orchestrator — the response payload will be empty or an error.
**Do this instead:** Always use `InvocationType="Event"`, then poll `list_durable_executions_by_function` for completion. See Pattern 5 above.

### Anti-Pattern 4: Generating Handler Stubs Over User-Authored Handlers

**What people do:** Run `rsf generate` in examples/ and let it overwrite the carefully written handler files.
**Why it's wrong:** The Generation Gap pattern in `codegen/generator.py` protects files without the generated marker. User handler files don't have the marker. But if someone accidentally deletes handlers/ and regenerates, the stubs are empty `NotImplementedError` placeholders, which will fail at runtime.
**Do this instead:** In the `run_integration_tests.sh` script, only run `rsf generate` to regenerate `orchestrator.py`. Never run it in a way that would reach the handlers directory. Separate the two directories clearly: `src/generated/` for generated output, `handlers/` for user code.

### Anti-Pattern 5: Using CloudWatch Logs as the Only Verification Channel

**What people do:** Skip asserting on the Lambda return value and rely only on CloudWatch Logs.
**Why it's wrong:** CloudWatch Logs delivery has eventual-consistency semantics — logs may appear seconds after execution completes. If the test queries logs immediately after polling completion, it may find no entries. Also, logs don't directly prove the workflow output is correct.
**Do this instead:** Always assert the Lambda return value first (it's synchronous relative to execution completion). Use CloudWatch Logs as a secondary verification for intermediate state and handler behavior.

## Integration Points

### External Services

| Service | Integration Pattern | Notes |
|---------|---------------------|-------|
| AWS Lambda | `boto3.client("lambda")` — `invoke` (Event), `list_durable_executions_by_function`, `get_durable_execution` | Existing `rsf.inspect.client.LambdaInspectClient` can be reused for polling; it has a built-in token bucket rate limiter |
| AWS CloudWatch Logs | `boto3.client("logs")` — `filter_log_events` with filterPattern for structured JSON | Use `{ $.execution_id = "..." }` pattern; retry 3x with 2s sleep for log delivery lag |
| AWS DynamoDB | `boto3.client("dynamodb")` — used directly in handler code for 1-2 examples | DynamoDB IAM permissions must be added to the example's `iam.tf` (override the generated file by removing the generated marker from line 1) |
| Terraform CLI | `subprocess.run(["terraform", ...])` — init, apply, output, destroy | Already used by `rsf.cli.deploy_cmd` — test harness uses the same pattern |

### Internal Boundaries

| Boundary | Communication | Notes |
|----------|---------------|-------|
| `examples/<name>/workflow.yaml` → `rsf deploy` | Existing CLI invocation via subprocess or direct Python API | Same interface as any user project — no special integration needed |
| `rsf.codegen.generator.generate()` | Called programmatically in test setup to regenerate orchestrator | Same function used by `rsf generate` CLI; call directly from Python in test conftest |
| `rsf.inspect.client.LambdaInspectClient` | Reuse for polling execution status | Already has async interface and rate limiter; wrap with `asyncio.run()` in synchronous pytest or use `pytest-asyncio` |
| `examples/<name>/handlers/` → Lambda package | Handlers must be importable as `handlers.<module>` at Lambda runtime | The `src/` layout ensures `src/handlers/` is in the package; `orchestrator.py` imports them as `import handlers.<module>` |
| `terraform output -json` → test conftest | subprocess call to read deployed resource names | Use `pathlib.Path` to locate the correct `terraform/` directory per example |

### New vs Modified Components

**New (to be created):**
- `examples/` directory tree with 3-4 example subdirectories
- `examples/<name>/workflow.yaml` (hand-authored DSL for each scenario)
- `examples/<name>/handlers/` (user-authored handler implementations)
- `examples/<name>/src/` layout structure
- `examples/<name>/tests/test_local.py` (local mock SDK tests per example)
- `tests/test_examples/conftest.py` (shared AWS test fixtures and helpers)
- `tests/test_examples/test_<name>.py` (one per example)
- `scripts/run_integration_tests.sh` (lifecycle orchestration)

**Modified (minimal changes to existing code):**
- `pyproject.toml` or `pytest.ini` — add `tests/test_examples` marker for AWS integration tests so they are excluded from default `pytest` runs (use `-m integration` marker)
- `.gitignore` — add `examples/*/terraform/terraform.tfstate`, `examples/*/src/generated/`, `examples/*/terraform/.terraform/`

**Not modified:**
- `src/rsf/` — all existing CLI, codegen, terraform, and inspect modules
- `src/rsf/terraform/templates/` — existing Jinja2 templates are correct for the example layout
- `tests/test_integration/` — existing mock SDK integration tests unchanged
- `tests/mock_sdk.py` — reusable as-is for local tests in each example

### Build Order Considering Dependencies

```
Phase A: Create examples (no AWS required)
  1. Write workflow.yaml for each example (RSF DSL)
  2. Run rsf generate locally to produce orchestrator.py (verifies DSL is valid)
  3. Write handler implementations with structured logging
  4. Write test_local.py per example (uses existing MockDurableContext)
  5. Run pytest tests/ (existing + new local tests) — must pass before any AWS work

Phase B: AWS test infrastructure
  6. Write conftest.py with polling helpers and fixture functions
  7. Write test_<name>.py files with assertions (before deploying — TDD)
  8. Write run_integration_tests.sh skeleton

Phase C: Deploy and verify (requires AWS credentials)
  9. rsf deploy each example with -y flag
  10. Read terraform output to verify resource names
  11. Run pytest tests/test_examples/ -m integration
  12. Iterate on handler code or test assertions
  13. terraform destroy each example after verification
```

The ordering rule: **never write AWS test assertions until the corresponding example
has passing local (mock SDK) tests.** Local tests are the confidence gate before
AWS deployment, which has real cost and latency.

## Sources

- Direct inspection of `/home/esa/git/rsf-python/src/rsf/` source tree
- `src/rsf/terraform/templates/main.tf.j2` — Lambda package path, handler module name
- `src/rsf/terraform/templates/cloudwatch.tf.j2` — log group naming convention
- `src/rsf/terraform/templates/outputs.tf.j2` — exported output names
- `src/rsf/terraform/generator.py` — TerraformConfig structure, source_dir default
- `src/rsf/cli/deploy_cmd.py` — deploy pipeline, tf_dir default, subprocess pattern
- `src/rsf/cli/generate_cmd.py` — codegen invocation, output_dir, handlers_dir
- `src/rsf/codegen/generator.py` — Generation Gap pattern, handler discovery
- `src/rsf/inspect/client.py` — LambdaInspectClient, token bucket rate limiter
- `src/rsf/inspect/models.py` — ExecutionStatus, TERMINAL_STATUSES
- `tests/test_integration/test_sdk_integration.py` — existing _build_and_exec pattern
- `tests/mock_sdk.py` — MockDurableContext implementation
- `.planning/PROJECT.md` — v1.2 milestone scope, Lambda invocation semantics

---
*Architecture research for: RSF v1.2 Comprehensive Examples and Integration Testing*
*Researched: 2026-02-26*
