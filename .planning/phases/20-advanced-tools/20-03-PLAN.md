---
phase: 20-advanced-tools
plan: 03
type: execute
wave: 1
depends_on: []
files_modified:
  - tutorials/08-execution-inspector.md
autonomous: true
requirements:
  - VIS-02
  - VIS-03

must_haves:
  truths:
    - "User can deploy a dedicated inspection workflow to AWS using the provided Terraform and have a running target for the inspector"
    - "User can launch rsf inspect, attach to a live execution, and see execution history with state-by-state progress"
    - "User can scrub through historical execution states with the time machine and observe structural data diffs between steps"
    - "User can tear down the inspection infrastructure cleanly with zero orphaned resources"
  artifacts:
    - path: "tutorials/08-execution-inspector.md"
      provides: "Step-by-step tutorial for deploying an inspection target, launching rsf inspect, time machine scrubbing, and teardown"
      min_lines: 280
  key_links:
    - from: "tutorials/08-execution-inspector.md"
      to: "src/rsf/cli/inspect_cmd.py"
      via: "Documents the exact behavior of rsf inspect"
      pattern: "rsf inspect"
    - from: "tutorials/08-execution-inspector.md"
      to: "src/rsf/inspect/server.py"
      via: "Documents the inspector FastAPI server and SSE streaming"
      pattern: "launch"
    - from: "tutorials/08-execution-inspector.md"
      to: "src/rsf/inspect/client.py"
      via: "Documents the Lambda durable execution API interactions"
      pattern: "LambdaInspectClient"
    - from: "tutorials/08-execution-inspector.md"
      to: "src/rsf/cli/deploy_cmd.py"
      via: "Inspection target deployed via rsf deploy"
      pattern: "rsf deploy"
---

<objective>
Create a comprehensive step-by-step tutorial that covers deploying a dedicated inspection workflow to AWS and using `rsf inspect` to attach to live executions, scrub through execution history with the time machine, and observe state-by-state data flow.

Purpose: Users need a hands-on guide to deploy a workflow specifically for inspection, launch the execution inspector, watch a live execution in progress, use the time machine to scrub through historical states, and cleanly tear down the inspection infrastructure.
Output: `tutorials/08-execution-inspector.md` — a complete tutorial document.
</objective>

<execution_context>
@/home/esa/.claude/get-shit-done/workflows/execute-plan.md
@/home/esa/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

@src/rsf/cli/inspect_cmd.py
@src/rsf/cli/deploy_cmd.py
@src/rsf/inspect/server.py
@src/rsf/inspect/client.py
@src/rsf/inspect/router.py
@src/rsf/inspect/models.py
@tutorials/04-deploy-to-aws.md
@tutorials/05-iterate-invoke-teardown.md

<interfaces>
<!-- The rsf inspect command behavior (from src/rsf/cli/inspect_cmd.py):

  Options:
    --arn: Lambda function ARN to inspect (default: None — auto-discovered)
    --port / -p: Port to serve on (default: 8766)
    --no-browser: Don't auto-open browser (default: False)
    --tf-dir: Terraform directory for ARN discovery (default: "terraform")

  Behavior:
  1. If --arn not provided:
     a. Check terraform/terraform.tfstate exists
     b. Run: terraform output -raw function_arn (in tf_dir)
     c. Use the output as the ARN
     d. If discovery fails: "Error: No --arn provided and could not discover ARN from Terraform state. Use --arn <function-arn>."
  2. Prints: "Starting RSF Inspector on port {port}..."
  3. Prints: "Inspecting: {resolved_arn}"
  4. Creates FastAPI app with:
     - GET /api/inspect/executions — list durable executions (with status filter, pagination)
     - GET /api/inspect/execution/{id} — execution detail + history
     - GET /api/inspect/execution/{id}/history — history events only
     - GET /api/inspect/execution/{id}/stream — SSE live stream
     - Static files (React SPA at / if build exists)
  5. Opens browser to http://127.0.0.1:{port}
  6. Runs until Ctrl+C → prints "Server stopped"

  Inspector SSE stream lifecycle:
  1. Connect → sends execution_info + full history
  2. Polls every 5s → sends execution_info; if new events, sends history_update
  3. Closes when execution reaches terminal status (SUCCEEDED, FAILED, TIMED_OUT, STOPPED)

  Execution statuses: RUNNING, SUCCEEDED, FAILED, TIMED_OUT, STOPPED

  History events contain: event_id, timestamp, event_type, sub_type, details (dict)

  The inspector UI (React SPA) provides:
  - Execution list panel: shows all durable executions with status, start time
  - Execution detail panel: input payload, output/result, error info, history timeline
  - Time machine scrubber: slider to scrub through history events and see state-by-state data
  - Live updates: SSE streaming shows execution progress in real time for RUNNING executions
  - Data diff view: structural diff between consecutive history event details
-->
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Write the inspection deployment section of the tutorial</name>
  <files>tutorials/08-execution-inspector.md</files>
  <action>
Create the file `tutorials/08-execution-inspector.md` with the first half covering deployment of the inspection target.

**Title:** "Tutorial 8: Execution Inspection with `rsf inspect`"

**Sections to include (in order):**

1. **What You'll Learn** — Brief overview: deploy a workflow to AWS for inspection, launch the execution inspector, view a list of executions, attach to a live execution, use the time machine to scrub through state history, observe data diffs between steps, and tear down all resources.

2. **Prerequisites** —
   - Completed Tutorials 1-5 (familiar with the RSF workflow: init, validate, generate, deploy, invoke, teardown)
   - AWS CLI installed and configured with credentials
   - Terraform CLI installed (version 1.0+)
   - An AWS account with us-east-2 region enabled
   - Show verification commands:
     ```bash
     aws sts get-caller-identity
     terraform --version
     ```

3. **Step 1: Create the Inspection Workflow** — Create a new project directory for the inspection tutorial:
   ```bash
   mkdir inspect-demo && cd inspect-demo
   rsf init .
   ```
   Replace workflow.yaml with a multi-state workflow that processes data through several steps (to make the execution history interesting to inspect). Use this workflow:
   ```yaml
   rsf_version: "1.0"
   Comment: "Inspection demo workflow"
   StartAt: FetchData

   States:
     FetchData:
       Type: Task
       Next: TransformData

     TransformData:
       Type: Task
       Next: CheckQuality

     CheckQuality:
       Type: Choice
       Choices:
         - Variable: "$.quality_score"
           NumericGreaterThanOrEquals: 80
           Next: PublishResults
       Default: FlagForReview

     FlagForReview:
       Type: Task
       Next: PublishResults

     PublishResults:
       Type: Task
       End: true
   ```
   Explain: this workflow has four Task states and one Choice state, giving us multiple history events to inspect. The Choice at `CheckQuality` routes based on a quality score, so we can trigger different execution paths.

4. **Step 2: Implement the Handlers** — Show handler implementations for all four Task states. Each handler should add meaningful data to the output so the inspector shows interesting state-by-state data transformations:

   `handlers/fetch_data.py`:
   ```python
   from rsf.registry import state

   @state("FetchData")
   def fetch_data(input_data: dict) -> dict:
       """Simulate fetching data from a source."""
       record_id = input_data.get("record_id", "R-001")
       return {
           "record_id": record_id,
           "raw_data": {"temperature": 72.5, "humidity": 45},
           "source": "sensor-array-01",
           "fetched": True,
       }
   ```

   `handlers/transform_data.py`:
   ```python
   from rsf.registry import state

   @state("TransformData")
   def transform_data(input_data: dict) -> dict:
       """Transform raw data and compute quality score."""
       raw = input_data.get("raw_data", {})
       temp_c = (raw.get("temperature", 0) - 32) * 5 / 9
       return {
           **input_data,
           "transformed": {
               "temperature_celsius": round(temp_c, 1),
               "humidity_pct": raw.get("humidity", 0),
           },
           "quality_score": 85 if temp_c > 15 else 60,
       }
   ```

   `handlers/flag_for_review.py`:
   ```python
   from rsf.registry import state

   @state("FlagForReview")
   def flag_for_review(input_data: dict) -> dict:
       """Flag low-quality data for manual review."""
       return {
           **input_data,
           "flagged": True,
           "review_reason": "Quality score below threshold",
       }
   ```

   `handlers/publish_results.py`:
   ```python
   from rsf.registry import state

   @state("PublishResults")
   def publish_results(input_data: dict) -> dict:
       """Publish final results."""
       return {
           "record_id": input_data.get("record_id"),
           "status": "published",
           "quality_score": input_data.get("quality_score"),
           "flagged": input_data.get("flagged", False),
       }
   ```

5. **Step 3: Deploy the Inspection Workflow** — Deploy to AWS:
   ```bash
   rsf deploy --auto-approve
   ```
   Show expected output (code generation + Terraform apply). The workflow is now live and ready to be invoked and inspected.

   > **Cost reminder:** This creates real AWS resources. Follow the teardown steps at the end to avoid ongoing charges.

6. **Step 4: Invoke the Workflow** — Invoke twice with different inputs to create execution history:
   ```bash
   aws lambda invoke \
     --function-name rsf-Inspection-demo-workflow \
     --region us-east-2 \
     --payload '{"record_id": "R-001"}' \
     --cli-binary-format raw-in-base64-out \
     response1.json

   cat response1.json
   ```
   Then invoke with different data to trigger the other Choice branch:
   ```bash
   aws lambda invoke \
     --function-name rsf-Inspection-demo-workflow \
     --region us-east-2 \
     --payload '{"record_id": "R-002", "raw_data": {"temperature": 40, "humidity": 90}}' \
     --cli-binary-format raw-in-base64-out \
     response2.json

   cat response2.json
   ```
   Explain: the first invocation uses default sensor data (temperature 72.5F = 22.5C > 15, quality_score 85 >= 80) so it goes through FetchData → TransformData → CheckQuality → PublishResults. The second invocation provides cold temperature data (40F = 4.4C < 15, quality_score 60 < 80) so it goes through FetchData → TransformData → CheckQuality → FlagForReview → PublishResults.

   Now we have two completed executions with different paths to inspect.
  </action>
  <verify>
    <automated>echo "Task 1 creates the file; verification runs after Task 2 adds remaining content"</automated>
  </verify>
  <done>
    - tutorials/08-execution-inspector.md created with inspection workflow deployment sections
    - Multi-state workflow defined with meaningful data transformations
    - Four handler implementations with progressive data enrichment
    - Two invocations triggering different Choice branches
    - Cost warning included
  </done>
</task>

<task type="auto">
  <name>Task 2: Write the rsf inspect and teardown sections of the tutorial</name>
  <files>tutorials/08-execution-inspector.md</files>
  <action>
Continue the file `tutorials/08-execution-inspector.md` by appending the remaining sections (Steps 5-9). Do NOT overwrite the content from Task 1 — append after Step 4.

**Sections to append (continuing from Step 4):**

7. **Step 5: Launch the Inspector** — Show the exact command:
   ```bash
   rsf inspect
   ```
   Show expected terminal output:
   ```
   Starting RSF Inspector on port 8766...
   Inspecting: arn:aws:lambda:us-east-2:123456789012:function:rsf-Inspection-demo-workflow
   ```
   Explain:
   - The inspector auto-discovers the Lambda function ARN from the Terraform state in `terraform/`. No need to provide `--arn` if you have a `terraform/` directory.
   - The browser opens to `http://127.0.0.1:8766`
   - If Terraform state is not available, provide the ARN explicitly: `rsf inspect --arn arn:aws:lambda:us-east-2:123456789012:function:rsf-Inspection-demo-workflow`
   - Use `--port` to change the port: `rsf inspect --port 9000`

   > **Tip:** Keep the terminal with `rsf inspect` running while you explore. Press Ctrl+C to stop the server when done.

8. **Step 6: Explore the Execution List** — Describe what the user sees in the browser:
   - The execution list panel shows all durable executions for this Lambda function
   - Each execution shows: execution ID, status (SUCCEEDED, FAILED, RUNNING, etc.), start time, end time (if completed)
   - The two invocations from Step 4 should appear as SUCCEEDED executions
   - Click an execution to view its detail

   Explain the execution statuses:
   - **RUNNING**: Execution is in progress (the inspector streams live updates via SSE)
   - **SUCCEEDED**: Execution completed successfully
   - **FAILED**: Execution encountered an error
   - **TIMED_OUT**: Execution exceeded the configured timeout
   - **STOPPED**: Execution was manually stopped

9. **Step 7: Inspect an Execution with the Time Machine** — Walk through selecting the first execution (R-001) and using the time machine:
   - **Execution detail** shows: input payload (`{"record_id": "R-001"}`), final result, and execution duration
   - **History timeline** shows each event in chronological order — state entries, state exits, data transformations
   - **Time machine scrubber** — a slider at the bottom lets you scrub through the execution history:
     - Move the slider to the first event: see the initial input to FetchData
     - Move to the second event: see the output of FetchData (raw sensor data added)
     - Move to the third event: see the output of TransformData (celsius conversion, quality_score computed)
     - Move to the fourth event: see the CheckQuality decision (quality_score 85 >= 80, routing to PublishResults)
     - Move to the final event: see the output of PublishResults (final result)
   - **Data diff view** — as you scrub between events, the inspector highlights structural differences between consecutive states:
     - Between FetchData output and TransformData output: the `transformed` field was added, `quality_score` was computed
     - Between TransformData output and PublishResults output: the data was summarized to just `record_id`, `status`, `quality_score`, `flagged`

   Explain: the time machine lets you "replay" the execution step by step, seeing exactly what data flowed between states. This is invaluable for debugging — you can pinpoint exactly where data was transformed incorrectly or where a Choice condition evaluated unexpectedly.

10. **Step 8: Compare Execution Paths** — Select the second execution (R-002) and use the time machine to see the different path:
    - This execution went through FlagForReview (because quality_score 60 < 80)
    - The history has an extra event for FlagForReview
    - The data diff shows the `flagged: true` and `review_reason` fields added by FlagForReview
    - The final PublishResults output includes `flagged: true`

    Explain: comparing executions with different paths helps you understand how your Choice conditions route data and verify that all branches produce correct output.

11. **Step 9: Live Inspection (Optional)** — For users who want to see live streaming:
    - Open the inspector in the browser (`rsf inspect` still running)
    - In a separate terminal, invoke the Lambda again:
      ```bash
      aws lambda invoke \
        --function-name rsf-Inspection-demo-workflow \
        --region us-east-2 \
        --payload '{"record_id": "R-003"}' \
        --cli-binary-format raw-in-base64-out \
        response3.json
      ```
    - In the browser, a new RUNNING execution appears in the list
    - Click it to see live updates via SSE — events appear in the timeline as they happen
    - Once the execution completes, the status changes to SUCCEEDED and the SSE stream closes

    Explain: the inspector uses Server-Sent Events (SSE) to stream execution updates every 5 seconds. For fast-completing executions, you may only see the final state. For long-running workflows, you can watch events arrive in real time.

12. **Step 10: Tear Down** — Stop the inspector (Ctrl+C in the inspector terminal), then tear down the AWS infrastructure:
    ```bash
    cd terraform && terraform destroy -auto-approve
    ```
    Verify clean teardown:
    ```bash
    aws lambda get-function --function-name rsf-Inspection-demo-workflow --region us-east-2
    ```
    Expected: ResourceNotFoundException — confirming zero orphaned resources.

    > **Important:** Always tear down inspection infrastructure when you are done. Lambda functions and CloudWatch log groups incur ongoing costs.

13. **What You've Learned** — Summarize the full RSF tutorial series:
    - Tutorial 1: `rsf init` — scaffold a project
    - Tutorial 2: `rsf validate` — validate workflow YAML
    - Tutorial 3: `rsf generate` — generate orchestrator + handler stubs
    - Tutorial 4: `rsf deploy` — deploy to AWS with Terraform
    - Tutorial 5: `rsf deploy --code-only` — iterate, invoke, tear down
    - Tutorial 6: `rsf import` — migrate from AWS Step Functions ASL
    - Tutorial 7: `rsf ui` — visually edit workflows
    - Tutorial 8: `rsf inspect` — inspect live executions

    The complete RSF workflow: define → validate → generate → deploy → iterate → inspect → tear down.

**Style guidelines (follow exactly):**
- Follow the established tutorial patterns from Tutorials 1-7
- Use clear, direct language. No marketing fluff.
- Every command should be in a fenced code block with `bash` syntax highlighting.
- Use blockquotes (> ) for important notes or tips, especially for cost warnings and teardown reminders.
- Keep explanations concise — 2-4 sentences per concept.
- Do NOT use emojis.
- Use us-east-2 as the AWS region throughout.
- Describe UI elements clearly for users who cannot see screenshots — this is a text-only tutorial.
- The function name pattern is `rsf-{Comment}` where Comment is from workflow.yaml (with spaces replaced by hyphens).
  </action>
  <verify>
    <automated>test -f tutorials/08-execution-inspector.md && wc -l tutorials/08-execution-inspector.md | awk '{if ($1 >= 280) print "PASS: "$1" lines"; else print "FAIL: only "$1" lines"}'</automated>
  </verify>
  <done>
    - tutorials/08-execution-inspector.md complete with 280+ lines
    - Tutorial covers: launching rsf inspect, execution list panel, execution detail with input/output, time machine scrubber with step-by-step walkthrough, data diff view between consecutive states, comparing two execution paths (different Choice branches), live SSE streaming for running executions, teardown with verification
    - ARN auto-discovery from Terraform state documented
    - Time machine demonstrated with concrete data flow (sensor data → transformation → quality check → publish)
    - Data diff explained with specific field-level changes
    - Cost warning and teardown at the end
    - Tutorial series summary included
  </done>
</task>

</tasks>

<verification>
1. `tutorials/08-execution-inspector.md` exists and has 280+ lines
2. Tutorial documents deploying a dedicated inspection workflow with rsf deploy
3. Two invocations trigger different Choice branches (quality_score 85 vs 60)
4. rsf inspect launch with ARN auto-discovery from Terraform state
5. Execution list, execution detail, and history timeline described
6. Time machine scrubber walkthrough with step-by-step data flow
7. Data diff view showing structural changes between consecutive states
8. Live SSE streaming demonstrated for running executions
9. Teardown via terraform destroy with ResourceNotFoundException verification
10. AWS region is us-east-2 throughout
11. Cost warnings present for live infrastructure
</verification>

<success_criteria>
A user reading this tutorial can: deploy a dedicated workflow for inspection, invoke it to create execution history, launch `rsf inspect`, browse executions, use the time machine to scrub through state history, observe data diffs between steps, watch live execution updates, and tear down all resources.
</success_criteria>

<output>
After completion, create `.planning/phases/20-advanced-tools/20-03-SUMMARY.md`
</output>
